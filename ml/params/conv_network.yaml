# run with python -m ml.train_nn_classification
experiment_name:
  default: 't5_cnn'
  help: 'experiment name'
run_name:
  default: 'cnn_ns_12'
  help: 'run name'
log_dir:
  default: ""
  help: "Use a custom root log directory where experiments and runs are then saved on path
  root_dir/exp_name/DATE_TIME_run_name. If string is empty, root_dir is set to PROJECT_ROOT/runs"
device:
  default: "cuda"
  help:
gpu_id:
  default: "0"
  help:
class_type:
  default: 2
  help: "3 class approach (Healthy , OR damage, IR damage) (type 1 : artificial damage),
         3 class approach (Healthy , OR damage, IR damage) (type 2 : real damage),
         5 class approach (type1 & type2)(type 3 : real & artificial damage),
         11 class approach (type1 & type2 & damage_extent) (type 4 : real & artificial & damage_extent)"
signal_type:
  default: "raw_vibration"
  help:
filter:
  default: "none"
  help:
bearing_name_train:
  default: [ "K001", "K002" , "K003", "K004",
             "KI04", "KI14", "KI16","KI18",
             "KA04", "KA15", "KA16","KA22" ]
  help: "Name of bearing for the training. it should be nested array to match the number of classes"
num_measurement_train:
  default: 20
  help: "number of measurements"
enable_testing:
  default: True
  help:
bearing_name_test:
  default: ["K005", "KI21", "KA30"]
  help: "Name of bearing for the testing. it should be nested array to match the number of classes"
num_measurement_test:
  default: 10
  help: "number of measurements"
segement_size:
  default: 2000
  help: "length of a segment"
norm_type_input:
  default: "minmax"
  help: "Input normalization type. Can be 'std' or 'minmax'"
train_dev_test_split:
  default: [0.8, 0.2, 0.0]
  help: ""
seed:
  default: 23423
  help: "Seed applied to all random generators"
convNet_type:
  default: "AutoEncoder"
  help:
conv_channels_en:
  default: [1, 1, 1, 1, 1, 1, 1, 1, 1]
  help:
conv_kernel_size_en:
  default: [5, 5, 5, 5, 5, 5, 5, 5]
  help:
conv_stride_en:
  default: [2, 2, 2, 2, 2, 2, 2, 2]
  help:
conv_padding_en:
  default: [1, 1, 1, 1, 1, 1, 1, 1]
  help:
conv_out_padding_en:
  default: [1, 1, 1, 1, 1, 1, 1, 1]
  help:
bottleneck_layer:
  default: [5]
  help:
en_functionals:
  default: [["relu", "relu", "relu", "relu", "relu", "relu", "relu", "relu"], ["relu"]]
  help:
en_batch_norm:
  default: [[True, True, True, True, True, True, True, True], [True]]
  help: "Adds batch normalization layer after each layer. Size of batch_norm should match the number of hidden layers.
  Disabled by default."
en_dropout:
  default: [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0]]
  help: Probability of the an element to be zeroed in each layer.
conv_channels_de:
  default: [1, 1, 1, 1, 1, 1, 1, 1, 1]
  help:
conv_kernel_size_de:
  default: [5, 5, 5, 5, 5, 5, 5, 5]
  help:
conv_stride_de:
  default: [2, 2, 2, 2, 2, 2, 2, 2]
  help:
conv_padding_de:
  default: [1, 1, 1, 1, 1, 1, 1, 1]
  help:
conv_out_padding_de:
  default: [1, 1, 1, 1, 1, 1, 1, 1]
  help:
output_layer:
  default: [128,64,32,3]
  help:
de_functionals:
  default: [["relu", "relu", "relu", "relu", "relu", "relu", "relu", "relu"], ["relu", "relu", "relu", "softmax"]]
  help:
de_batch_norm:
  default: [[True, True, True, True, True, True, True, True], [True,True,True, False]]
  help: "Adds batch normalization layer after each layer. Size of batch_norm should match the number of hidden layers.
  Disabled by default."
de_dropout:
  default: [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]]
  help: Probability of the an element to be zeroed in each layer.
num_epochs:
  default: 20
  help: "Number of epochs"
max_training_time:
  default: 0
  help: "Maximum training time in hours. Disabled by default (default=0)"
batch_size:
  default: 64
  help: "Size of the training batch"
lr:
  default: 1e-4
  help: "Learning rate"
weight_decay:
  default: 1e-5
  help: "Learning rate decay"
eps:
  default: 1e-5
  help: "A value added to the denominator for numerical stability. Used when batch normalization is applied."
momentum:
  default: 0.1
  help: "The value used for the running_mean and running_var computation. Can be set to ``None`` for cumulative
  moving average (i.e. simple average). Used when batch normalization is applied."
optim:
  default: "Adam"
  help: "Name of the optimizer. Can be any name that matches an optimizer from torch.optim*"
loss:
  default: "CrossEntropyLoss"
  help: "Name of the metric to calculate the loss. Can be any valid metric name from torch.nn*"
log_tensorboard:
  default: True
  help: